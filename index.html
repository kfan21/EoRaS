<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation.">
  <meta name="keywords" content="Amodal Segementation, Amodal Video Segmentation, Object-centric Learning, Slot Attention">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jianxgao.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation</h1>
          <h2><font color="gray" size="5">ICCV 2023</font></h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kfan21.github.io/">Ke Fan</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/cc-crow">Jingshi Lei</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://naiq.github.io/">Xuelin Qian</a><sup>1,†</sup>,
            </span>
            <span class="author-block">
              <a>Miaopeng Yu</a><sup>1</sup>,
            </span><br />
            <span class="author-block">
              <a href="http://tianjunxiao.com/">Tianjun Xiao</a><sup>2,†</sup>,
            </span>
            <span class="author-block">
              <a href="https://hetong007.github.io/">Tong He</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=k0KiE4wAAAAJ">Zheng Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://yanweifu.github.io/">Yanwei Fu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University, <sup>2</sup>Amazon Web Service</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2309.13248"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/amazon-science/efficient-object-centric-representation-amodal-segmentation"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
             <span class="link-block">
               <a href="https://data.dgl.ai/dataset/EoRas/MOVi-B.tar.gz"
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fas fa-database"></i>
                 </span>
                 <span>MOVi-B</span>
                 </a>
             </span>
             <span class="link-block">
              <a href="https://data.dgl.ai/dataset/EoRas/MOVi-D.tar.gz"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-database"></i>
                </span>
                <span>MOVi-D</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video amodal segmentation is a particularly challenging task in computer vision, which requires to deduce the full shape of an object from the visible parts of it.
            Recently, some studies have achieved promising performance by using motion flow to integrate information across frames under a self-supervised setting.
            However, motion flow has a clear limitation by the two factors of moving cameras and object deformation.
            This paper presents a rethinking to previous works.
            We particularly leverage the supervised signals with object-centric representation in real-world scenarios.
            The underlying idea is the supervision signal of the specific object and the features from different views can mutually benefit the deduction of the full mask in any specific frame.
            We thus propose an Efficient object-centric Representation amodal Segmentation (EoRaS).
            Specially, beyond solely relying on supervision signals, we design a translation module to project image features into the Bird's-Eye View (BEV), which introduces 3D information to improve current feature quality. 
            Furthermore, we propose a multi-view fusion layer based temporal module which is equipped with a set of object slots and interacts with features from different views by attention mechanism to fulfill sufficient object representation completion. As a result, the full mask of the object can be decoded from image features updated by object slots.
            Extensive experiments on both real-world and synthetic benchmarks demonstrate the superiority of our proposed method, achieving state-of-the-art performance. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <h2 class="title is-3">Illustrations of view prior,
          shape prior, and our model</h2>
        <img src="./imgs/teaser.jpg" width="50%"/>
      </center>
      <p>
        Illustrations of the difference between view prior,
shape prior, and our model. Unlike the previous methods, beyond the mergence of those two priors, EoRaS utilizes view prior by object-centric learning and further introduces the BEV space where obstruction doesn’t exist, which
enables our EoRaS to easily handle complex scenarios.
      </p>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <h2 class="title is-3">Model Pipeline</h2>
        <img src="./imgs/framework.jpg" width="100%"/>
      </center>
      <p>
        <strong>EoRaS</strong> (<strong>E</strong>fficient <strong>o</strong>bject-centric <strong>R</strong>epresentation <strong>a</strong>modal <strong>S</strong>egmentation) is a framework designed for supervised video amodal segementation. The videos are first encoded by a convolutional neural network to get the front-view features. Then a translation module is used to project front-view features into the Bird’s-Eye View (BEV), which introduces 3D information to improve current feature quality. The front-view features and BEV features across the frames are integrated by a multi-view fusion layer based temporal module which is equipped with a set of object slots and interacts with features from different views by attention mechanism. Finally, the integrated front-view features are decoded into the visible and amodal masks.
      </p>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    
    <div class="hero-body">
      <center>
        <h2 class="title is-3">Results</h2>
        <img src="./imgs/22-385-32-79-88-compressed.gif" width="90%"/>
        <p>Comparision between the prediction(top row) of EoRaS and ground truth amodal mask(bottom row)</p>
      </center>
    </div>
  </div>
</section>







<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @InProceedings{Fan_2023_ICCV,
        author    = {Fan, Ke and Lei, Jingshi and Qian, Xuelin and Yu, Miaopeng and Xiao, Tianjun and He, Tong and Zhang, Zheng and Fu, Yanwei},
        title     = {Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation},
        booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
        month     = {October},
        year      = {2023},
        pages     = {1272-1281}
    }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2309.13248">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/amazon-science/efficient-object-centric-representation-amodal-segmentation" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
